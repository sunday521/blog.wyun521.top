<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0"><title>Python爬虫 | 星期天の博客</title><meta name="author" content="星期天"><meta name="copyright" content="星期天"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="ffffff"><meta name="description" content="爬虫是通过编写程序，模拟浏览器上网，并抓取有价值的数据的过程">
<meta property="og:type" content="article">
<meta property="og:title" content="Python爬虫">
<meta property="og:url" content="https://blog.wyun521.top/posts/c386cf1b/index.html">
<meta property="og:site_name" content="星期天の博客">
<meta property="og:description" content="爬虫是通过编写程序，模拟浏览器上网，并抓取有价值的数据的过程">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://mysource-hexo.wyun521.top/img/cover/s9.jpg">
<meta property="article:published_time" content="2023-01-13T03:50:13.000Z">
<meta property="article:modified_time" content="2023-02-04T14:41:20.875Z">
<meta property="article:author" content="星期天">
<meta property="article:tag" content="python">
<meta property="article:tag" content="爬虫">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://mysource-hexo.wyun521.top/img/cover/s9.jpg"><link rel="shortcut icon" href="https://mysource-hexo.wyun521.top/img/cover/logo.png"><link rel="canonical" href="https://blog.wyun521.top/posts/c386cf1b/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: {"appId":"7KX783XB76","apiKey":"295365855c37847d42f0eab77b287be0","indexName":"hexo-index","hits":{"per_page":10},"languages":{"input_placeholder":"搜索文章","hits_empty":"找不到您查询的内容：${query}","hits_stats":"找到 ${hits} 条结果，用时 ${time} 毫秒"}},
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":330},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: true,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  }
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Python爬虫',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-02-04 22:41:20'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = url => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      link.onload = () => resolve()
      link.onerror = () => reject()
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', 'ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="https://cdn1.tianli0.top/npm/element-ui@2.15.6/packages/theme-chalk/lib/index.css"><link rel="stylesheet" href="/css/custom@1.0/sunday.css"><!-- hexo injector head_end start --><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-wowjs/lib/animate.min.css" media="print" onload="this.media='screen'"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiperstyle.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-tag-plugins-plus@latest/lib/assets/font-awesome-animation.min.css" media="defer" onload="this.media='all'"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-tag-plugins-plus@latest/lib/tag_plugins.css" media="defer" onload="this.media='all'"><script src="https://npm.elemecdn.com/hexo-butterfly-tag-plugins-plus@latest/lib/assets/carousel-touch.js"></script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://mysource-hexo.wyun521.top/img/avatar/7.jpg" onerror="onerror=null;src='/img/favicon.png'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">18</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">10</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page group hide" href="javascript:void(0);"><i class="fa-fw fas fa-book"></i><span> 文章</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><a class="site-page group hide" href="javascript:void(0);"><i class="fa-fw fas fa-coffee"></i><span> 生活</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/photos/"><i class="fa-fw fas fa-images"></i><span> 图库</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://hin.cool/c/dd.html"><i class="fa-fw fas fa-fish"></i><span> 滑碟</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/links/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" target="_blank" rel="noopener" href="https://nav.wyun521.top/"><i class="fa-fw fas fa-clone"></i><span> 网址导航</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://mysource-hexo.wyun521.top/img/cover/s9.jpg')"><nav class="fixed" id="nav"><span id="blog-info"><a href="/" title="星期天の博客"><span class="site-name">星期天の博客</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page group hide" href="javascript:void(0);"><i class="fa-fw fas fa-book"></i><span> 文章</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><a class="site-page group hide" href="javascript:void(0);"><i class="fa-fw fas fa-coffee"></i><span> 生活</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/photos/"><i class="fa-fw fas fa-images"></i><span> 图库</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://hin.cool/c/dd.html"><i class="fa-fw fas fa-fish"></i><span> 滑碟</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/links/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" target="_blank" rel="noopener" href="https://nav.wyun521.top/"><i class="fa-fw fas fa-clone"></i><span> 网址导航</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Python爬虫</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-01-13T03:50:13.000Z" title="发表于 2023-01-13 11:50:13">2023-01-13</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-02-04T14:41:20.875Z" title="更新于 2023-02-04 22:41:20">2023-02-04</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Python/">Python</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">4k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>16分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Python爬虫"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div><section class="main-hero-waves-area waves-area"><svg class="waves-svg" xmlns="http://www.w3.org/2000/svg" xlink="http://www.w3.org/1999/xlink" viewBox="0 24 150 28" preserveAspectRatio="none" shape-rendering="auto"><defs><path id="gentle-wave" d="M -160 44 c 30 0 58 -18 88 -18 s 58 18 88 18 s 58 -18 88 -18 s 58 18 88 18 v 44 h -352 Z"></path></defs><g class="parallax"><use href="#gentle-wave" x="48" y="0"></use><use href="#gentle-wave" x="48" y="3"></use><use href="#gentle-wave" x="48" y="5"></use><use href="#gentle-wave" x="48" y="7"></use></g></svg></section></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h2 id="初识爬虫">初识爬虫</h2>
<h3 id="爬虫的概念">爬虫的概念</h3>
<p><strong>什么是爬虫</strong></p>
<p><code>爬虫</code>：通过编写程序，模拟浏览器上网，并抓取有价值的数据的过程</p>
<p><code>反爬虫</code>：门户网站通过制定相应的策略或技术手段，来阻止爬虫程序对其网站数据的爬取</p>
<p><code>反反爬</code>：爬虫程序可以采用一些技术手段，来绕过或破坏门户网站的反爬机制，从而爬取到有用的数据</p>
<blockquote>
<p>爬虫与反爬虫就是一对矛与盾</p>
</blockquote>
<p><strong>爬虫合法性探究</strong></p>
<p>爬虫可能带来的风险？</p>
<ul>
<li>爬虫干扰了被访问网站的正常运营</li>
<li>爬虫抓取了受到法律保护的特定类型的数据或信息</li>
</ul>
<p>如何合理地使用爬虫？</p>
<ul>
<li>对爬虫程序进行优化，避免干扰网站的正常运行</li>
<li>不要爬取涉及商业机密等敏感信息</li>
</ul>
<p><strong>爬虫的君子协议</strong></p>
<p>通常，网站的<code>robots.txt</code>文件中声明了那些数据可以被爬取，那些数据不可以被爬取（非强制性）</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.staticaly.com/gh/sunday521/postimg.wyun521.top@main/img/2023/20230113122447.png" alt="20230113122447"></p>
<h3 id="爬虫的分类">爬虫的分类</h3>
<p>在不同的使用场景下，爬虫的分类有</p>
<ol>
<li>
<p><code>通用爬虫</code>：抓取一整张页面的数据（很可能包含大量无用信息）</p>
</li>
<li>
<p><code>聚焦爬虫</code>：抓取页面中特定的局部内容，必须建立在通用爬虫的基础之上</p>
</li>
<li>
<p><code>增量爬虫</code>：只会爬取网站中最新更新的数据</p>
</li>
</ol>
<h2 id="网络请求与响应">网络请求与响应</h2>
<h3 id="http协议">http协议</h3>
<p>http(s)协议是服务器和客户端进行数据交互的一种形式，服务器和客户端都需要遵守该协议才能进行数据交互</p>
<p>https协议是http协议的升级版，服务器与客户端的数据交互是通过证书加密的，攻击者很难获得有价值的信息</p>
<p><strong>常用的请求头信息</strong></p>
<table>
<thead>
<tr>
<th>Request Header</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>User-Agent</td>
<td>请求载体的身份标识</td>
</tr>
<tr>
<td>Connection</td>
<td>请求完毕后，保持连接还是断开连接</td>
</tr>
</tbody>
</table>
<p><strong>常用的响应头信息</strong></p>
<table>
<thead>
<tr>
<th>Response Header</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>Content-Type</td>
<td>服务器响应数据的类型</td>
</tr>
</tbody>
</table>
<h3 id="requests模块">requests模块</h3>
<p><code>requests</code>是python中的一个基于网络请求的模块，用来模拟浏览器发送请求。</p>
<p><strong>requests模块的安装与使用</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install requests</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">url = <span class="string">&#x27;http://www.baidu.com&#x27;</span></span><br><span class="line">resp = requests.get(url)    <span class="comment">#发起一个get请求，并获得响应数据</span></span><br><span class="line">page_content = resp.text</span><br><span class="line"><span class="built_in">print</span>(page_content)</span><br></pre></td></tr></table></figure>
<table>
<thead>
<tr>
<th>属性</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>resp.text</td>
<td>以字符串形式返回，通常是页面的html源代码</td>
</tr>
<tr>
<td>resp.content</td>
<td>以二进制形式返回，比如一张图片、一个音频</td>
</tr>
<tr>
<td>resp.json()</td>
<td>返回一个字典对象（当响应数据是json类型时使用）</td>
</tr>
</tbody>
</table>
<p><strong>基于requests的简易网页采集器（使用到了UA伪装）</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. 准备数据</span></span><br><span class="line"><span class="comment"># url = &#x27;https://www.sogou.com/web?query=猫羽雫&#x27;</span></span><br><span class="line">url = <span class="string">&#x27;https://www.sogou.com/web&#x27;</span></span><br><span class="line">word = <span class="built_in">input</span>(<span class="string">&#x27;Enter a word:&#x27;</span>)</span><br><span class="line">params = &#123;  <span class="comment">#请求参数，拼接在url后</span></span><br><span class="line">    <span class="string">&#x27;query&#x27;</span>: word</span><br><span class="line">&#125;</span><br><span class="line">headers = &#123; <span class="comment">#请求头，伪装成浏览器</span></span><br><span class="line">    <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/107.0.0.0 Safari/537.36&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 发起请求</span></span><br><span class="line">resp = requests.get(url,params,headers=headers)</span><br><span class="line">resp.encoding = resp.apparent_encoding</span><br><span class="line">page_content = resp.text</span><br><span class="line"><span class="built_in">print</span>(page_content)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. 持久化存储</span></span><br><span class="line">file_name = word + <span class="string">&#x27;.html&#x27;</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(file_name,<span class="string">&#x27;w&#x27;</span>,encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> fp:</span><br><span class="line">    fp.write(page_content)</span><br><span class="line"><span class="built_in">print</span>(file_name,<span class="string">&#x27;保存成功！&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h2 id="数据解析">数据解析</h2>
<p><code>数据解析</code>是在得到整个网页源代码后，对其中的有用信息进行提取的过程。属于聚焦爬虫</p>
<p><strong>数据解析的一般步骤</strong></p>
<p>检查网页源代码发现，有价值的数据一般存放在标签中，或者标签的属性中。所以数据解析的一般步骤是：<mark>1.获取网页源代码 2.标签定位 3.解析数据</mark></p>
<blockquote>
<p>F12检查元素中的数据不一定在页面源代码中，也有可能是通过ajax动态刷新的数据，这是我们在数据解析时需要注意的。数据解析要以页面源代码为准！</p>
</blockquote>
<p><strong>Python中数据解析的三种方式</strong></p>
<p>1.正则表达式（通用）  2.BeautifulSoup4（python独有） 3.xpath（推荐，通用性最强）</p>
<h3 id="使用正则表达式">使用正则表达式</h3>
<p>建议先把要提取的那部分源码单独复制，对照着去写正则表达式（F12太乱了🤣）</p>
<p>从重复的标签（如li）开始写正则，那么这个正则可以提取到多组数据哦</p>
<p><strong>re.findall(pattern,string,flags)</strong></p>
<table>
<thead>
<tr>
<th>参数</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>pattern</td>
<td>匹配的正则表达式</td>
</tr>
<tr>
<td>string</td>
<td>待匹配的文本字符串</td>
</tr>
<tr>
<td>flags</td>
<td>标志位。用于控制正则表达式的匹配方式，如：是否区分大小写，多行匹配等等</td>
</tr>
</tbody>
</table>
<br/>
<table>
<thead>
<tr>
<th>标志位</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>re.S</td>
<td>使.能够匹配包括换行在内的所有字符</td>
</tr>
<tr>
<td>re.M</td>
<td>多行匹配，影响^和$</td>
</tr>
<tr>
<td>re.I</td>
<td>使匹配对大小写不敏感</td>
</tr>
</tbody>
</table>
<br/>
<table>
<thead>
<tr>
<th>正则表达式</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>.</td>
<td>匹配除换行符外的任意单个字符</td>
</tr>
<tr>
<td>*</td>
<td>匹配多个字符</td>
</tr>
<tr>
<td>?</td>
<td>非贪婪匹配</td>
</tr>
</tbody>
</table>
<h3 id="使用BeautifulSoup">使用BeautifulSoup</h3>
<p>对于一个网页来说，都有一定的特殊结构和层级关系，而且很多节点都用id和class来区分。所以可以借助网页的结构和属性来提取数据。</p>
<p><strong>使用BeautifulSoup的一般步骤</strong></p>
<ol>
<li>实例化一个BeautifulSoup对象，并且<mark>将页面源代码加载到该对象中</mark></li>
<li>调用BeautifulSoup对象提供的属性或方法进行标签定位和数据提取</li>
</ol>
<p><strong>BeautifulSoup4的安装与使用（需要一并下载lxml解析器）</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip install bs4</span><br><span class="line">pip install lxml</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载html有两种方式</span></span><br><span class="line"><span class="comment"># 方式一 使用本地html文档中的数据</span></span><br><span class="line">fp = <span class="built_in">open</span>(<span class="string">&#x27;./猫羽雫.html&#x27;</span>,<span class="string">&#x27;r&#x27;</span>,encoding=<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">soup = BeautifulSoup(fp,<span class="string">&#x27;lxml&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(soup) <span class="comment">#我们发现，soup对象的内容就是加载到该对象中的html源码</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 方式二 从互联网上获取html源码</span></span><br><span class="line">url = <span class="string">&#x27;https://www.baidu.com&#x27;</span></span><br><span class="line">resp = requests.get(url)</span><br><span class="line">resp.encoding = resp.apparent_encoding</span><br><span class="line">soup = BeautifulSoup(resp.text,<span class="string">&#x27;lxml&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(soup)</span><br></pre></td></tr></table></figure>
<p><strong>BeautifulSoup对象中提供的属性和方法</strong></p>
<ol>
<li>根据标签名或选择器定位，返回标签之间的所有内容</li>
</ol>
<table>
<thead>
<tr>
<th>方法</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>find(tag,attr=value)</td>
<td>根据标签名和属性进行定位，只返回符合条件的第一个元素内容</td>
</tr>
<tr>
<td>find_all(tag,attr=value)</td>
<td>返回一个列表，用法同上</td>
</tr>
<tr>
<td>select(css选择器)</td>
<td>使用CSS选择器进行定位，返回一个列表</td>
</tr>
</tbody>
</table>
<br/>
2. 获取标签之间的文本数据（不包括子标签）
<table>
<thead>
<tr>
<th>属性</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>.text</td>
<td>获取所有文本内容</td>
</tr>
<tr>
<td>.string</td>
<td>只能获取直系的文本内容</td>
</tr>
</tbody>
</table>
<br/>
3. 获取标签中指定属性的值
<table>
<thead>
<tr>
<th>属性</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>[attr]</td>
<td>获取属性对应的属性值</td>
</tr>
</tbody>
</table>
<h3 id="使用xpath解析">使用xpath解析</h3>
<p>xpath解析是最常用、最便捷、最高效，且通用性最强的解析方式。xpath是根据元素所处层级进行定位的</p>
<p><strong>xpath解析的一般步骤</strong></p>
<ol>
<li>实例化一个etree对象，并将要解析的html页面源码数据加载到对象中</li>
<li>调用etree对象的<code>xpath(xpath表达式)</code>方法结合xpath表达式实现标签的定位和数据提取</li>
</ol>
<p><strong>xpath的安装与使用</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install lxml</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载html有两种方式</span></span><br><span class="line"><span class="comment"># 方式一 使用本地html文档中的数据</span></span><br><span class="line">etree.parse(<span class="string">&#x27;./猫羽雫.html&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 方式二 从互联网上获取html源码</span></span><br><span class="line">url = <span class="string">&#x27;https://www.baidu.com&#x27;</span></span><br><span class="line">resp = requests.get(url)</span><br><span class="line">resp.encoding = resp.apparent_encoding</span><br><span class="line">etree.HTML(page_content)</span><br></pre></td></tr></table></figure>
<table>
<thead>
<tr>
<th>方法</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>etree.HTML(html)</td>
<td>实例化一个etree对象，并加载要解析的html</td>
</tr>
<tr>
<td>etree.parse(filepath)</td>
<td>实例化一个etree对象，并加载要解析的html（本地html）</td>
</tr>
<tr>
<td>xpath(xpath表达式)</td>
<td>使用xpath表达式进行标签定位，返回一个列表</td>
</tr>
</tbody>
</table>
<br/>
<p>xpath()方法返回一个列表，如果xpath表达式只进行了定位，没有进行数据提取，那么列表中每个元素都将是一个Element对象。</p>
<p><strong>xpath表达式用法</strong></p>
<ol>
<li>根据html元素层级进行定位</li>
</ol>
<table>
<thead>
<tr>
<th>xpath表达式</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>/</td>
<td>表示单个层级，放在开头表示html根元素</td>
</tr>
<tr>
<td>//</td>
<td>表示多个层级（常用）</td>
</tr>
<tr>
<td>./</td>
<td>表示当前标签</td>
</tr>
<tr>
<td>[@attr=value]</td>
<td>根据属性进行定位</td>
</tr>
<tr>
<td>[index]</td>
<td>根据元素所处位置进行定位，index从1开始</td>
</tr>
</tbody>
</table>
<br/>
<ol start="2">
<li>提取标签之间的文本数据</li>
</ol>
<table>
<thead>
<tr>
<th>xpath表达式</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>/text()</td>
<td>获取标签中的文本,返回一个列表</td>
</tr>
<tr>
<td>//text()</td>
<td>可以获取标签中非直系的文本，返回一个列表</td>
</tr>
</tbody>
</table>
<br/>
<ol start="3">
<li>提取标签中指定属性的值</li>
</ol>
<table>
<thead>
<tr>
<th>xpath表达式</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>/@attr</td>
<td>获取标签中属性对应的值</td>
</tr>
</tbody>
</table>
<br/>
<h2 id="数据解析实战">数据解析实战</h2>
<p>我们将通过三个案例分别使用三种数据解析方式获取到我们想要的数据</p>
<h3 id="彼岸图网">彼岸图网</h3>
<p><code>需求</code>：获取图中每个li标签内，所有a标签href属性的值，所有img标签src属性的值，以及所有b标签内的文本</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.staticaly.com/gh/sunday521/postimg.wyun521.top@main/img/2023/20230114184112.png" alt="20230114184112"></p>
<p><strong>方式一 正则表达式</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. 准备数据</span></span><br><span class="line">url = <span class="string">&#x27;https://pic.netbian.com/4kmeinv/&#x27;</span></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/105.0.0.0 Safari/537.36&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line">resp = requests.get(url,headers=headers)</span><br><span class="line">resp.encoding = resp.apparent_encoding</span><br><span class="line">page_content = resp.text</span><br><span class="line"></span><br><span class="line"><span class="comment"># 为了防止下一步获取的&lt;li&gt;标签不准确，这里加了一步先获取&lt;ul&gt;的操作</span></span><br><span class="line">ex0 = <span class="string">&#x27;&lt;ul class=&quot;clearfix&quot;&gt;(.*?)&lt;/ul&gt;&#x27;</span></span><br><span class="line">ul = re.findall(ex0,page_content,re.S)  <span class="comment">#实际上页面中就一个符合条件的ul</span></span><br><span class="line">ul_content = ul[<span class="number">0</span>].strip()  <span class="comment">#strip()去除字符串首尾空格和换行</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 数据解析</span></span><br><span class="line"><span class="comment"># 待匹配的字符串格式：&lt;li&gt;&lt;a href=&quot;/tupian/30912.html&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;/uploads/allimg/230111/012011-16733712117326.jpg&quot; alt=&quot;4k 居家 可爱 公主 女孩 美女 粉色裙子 电脑 壁纸&quot; /&gt;&lt;b&gt;4k 居家 可爱 公主 女孩&lt;/b&gt;&lt;/a&gt;&lt;/li&gt;</span></span><br><span class="line">ex = <span class="string">&#x27;&lt;li&gt;&lt;a href=&quot;(.*?)&quot; .*?&lt;img src=&quot;(.*?)&quot; .*?&lt;b&gt;(.*?)&lt;/b&gt;&lt;/a&gt;&lt;/li&gt;&#x27;</span> <span class="comment">#把想要提取的内容用()括起来</span></span><br><span class="line">data_list = re.findall(ex,ul_content,re.S)</span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> data_list:</span><br><span class="line">    <span class="built_in">print</span>(data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出结果</span></span><br><span class="line"><span class="comment"># (&#x27;/tupian/30912.html&#x27;, &#x27;/uploads/allimg/230111/012011-16733712117326.jpg&#x27;, &#x27;4k 居家 可爱 公主 女孩&#x27;)</span></span><br><span class="line"><span class="comment"># (&#x27;/tupian/30903.html&#x27;, &#x27;/uploads/allimg/230110/153047-16733358474a9c.jpg&#x27;, &#x27;甜美微笑 双手合十 小清&#x27;)</span></span><br><span class="line"><span class="comment"># ... ...</span></span><br></pre></td></tr></table></figure>
<p><strong>方式二 bs4</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. 准备数据</span></span><br><span class="line">url = <span class="string">&#x27;https://pic.netbian.com/4kmeinv/&#x27;</span></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/105.0.0.0 Safari/537.36&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line">resp = requests.get(url,headers=headers)</span><br><span class="line">resp.encoding = resp.apparent_encoding</span><br><span class="line">page_content = resp.text</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 数据解析</span></span><br><span class="line">soup = BeautifulSoup(page_content,<span class="string">&#x27;lxml&#x27;</span>)</span><br><span class="line">ul = soup.find(<span class="string">&#x27;ul&#x27;</span>,class_=<span class="string">&#x27;clearfix&#x27;</span>)</span><br><span class="line">a_list = ul.find_all(<span class="string">&#x27;a&#x27;</span>)</span><br><span class="line"><span class="comment"># 或者直接 a_list = soup.select(&#x27;div.clearfix li a&#x27;)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> a <span class="keyword">in</span> a_list:</span><br><span class="line">    a_href = a[<span class="string">&#x27;href&#x27;</span>]</span><br><span class="line">    <span class="built_in">print</span>(a_href)</span><br><span class="line"></span><br><span class="line">    img_src = a.find(<span class="string">&#x27;img&#x27;</span>)[<span class="string">&#x27;src&#x27;</span>]</span><br><span class="line">    <span class="built_in">print</span>(img_src)</span><br><span class="line"></span><br><span class="line">    b = a.find(<span class="string">&#x27;b&#x27;</span>).text    <span class="comment">#text获取标签中的所有文本</span></span><br><span class="line">    <span class="built_in">print</span>(b)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出结果</span></span><br><span class="line"><span class="comment"># /tupian/30912.html</span></span><br><span class="line"><span class="comment"># /uploads/allimg/230111/012011-16733712117326.jpg</span></span><br><span class="line"><span class="comment"># 4k 居家 可爱 公主 女孩</span></span><br><span class="line"><span class="comment"># /tupian/30903.html</span></span><br><span class="line"><span class="comment"># /uploads/allimg/230110/153047-16733358474a9c.jpg</span></span><br><span class="line"><span class="comment"># 甜美微笑 双手合十 小清</span></span><br><span class="line"><span class="comment"># ... ...</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><strong>方式三 xpath</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. 准备数据</span></span><br><span class="line">url = <span class="string">&#x27;https://pic.netbian.com/4kmeinv/&#x27;</span></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/105.0.0.0 Safari/537.36&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line">resp = requests.get(url,headers=headers)</span><br><span class="line">resp.encoding = resp.apparent_encoding</span><br><span class="line">page_content = resp.text</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 数据解析</span></span><br><span class="line"><span class="comment"># //多个层级下找  xpath()永远返回一个列表</span></span><br><span class="line">tree = etree.HTML(page_content)</span><br><span class="line">li_list = tree.xpath(<span class="string">&#x27;//ul[@class=&quot;clearfix&quot;]/li&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> li <span class="keyword">in</span> li_list:</span><br><span class="line">    a_href = li.xpath(<span class="string">&#x27;./a/@href&#x27;</span>)[<span class="number">0</span>]</span><br><span class="line">    <span class="built_in">print</span>(a_href)</span><br><span class="line"></span><br><span class="line">    img_src = li.xpath(<span class="string">&#x27;.//img/@src&#x27;</span>)[<span class="number">0</span>]</span><br><span class="line">    <span class="built_in">print</span>(img_src)</span><br><span class="line"></span><br><span class="line">    span = li.xpath(<span class="string">&#x27;.//b/text()&#x27;</span>)[<span class="number">0</span>]</span><br><span class="line">    <span class="built_in">print</span>(span)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出结果</span></span><br><span class="line"><span class="comment"># /tupian/30912.html</span></span><br><span class="line"><span class="comment"># /uploads/allimg/230111/012011-16733712117326.jpg</span></span><br><span class="line"><span class="comment"># 4k 居家 可爱 公主 女孩</span></span><br><span class="line"><span class="comment"># /tupian/30903.html</span></span><br><span class="line"><span class="comment"># /uploads/allimg/230110/153047-16733358474a9c.jpg</span></span><br><span class="line"><span class="comment"># 甜美微笑 双手合十 小清</span></span><br><span class="line"><span class="comment"># ... ...</span></span><br></pre></td></tr></table></figure>
<h3 id="Wallhaven">Wallhaven</h3>
<p><code>需求</code>：获取图中每个li标签内，所有a标签href属性的值，所有img标签data-src属性的值，以及所有span标签内图片的分辨率信息</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.staticaly.com/gh/sunday521/postimg.wyun521.top@main/img/2023/20230114194136.png" alt="20230114194136"></p>
<p><strong>方式一 正则表达式</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. 准备数据</span></span><br><span class="line">url = <span class="string">&#x27;https://wallhaven.cc/toplist&#x27;</span></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/105.0.0.0 Safari/537.36&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line">resp = requests.get(url,headers=headers)</span><br><span class="line">resp.encoding = resp.apparent_encoding</span><br><span class="line">page_content = resp.text</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 数据解析</span></span><br><span class="line">ex = <span class="string">&#x27;&lt;img .*? data-src=&quot;(.*?)&quot; .*?&lt;a .*? href=&quot;(.*?)&quot; .*?&lt;span .*?&gt;(.*?)&lt;/span&gt;&#x27;</span></span><br><span class="line">data_list = re.findall(ex,page_content,re.S)</span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> data_list:</span><br><span class="line">    <span class="built_in">print</span>(data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出结果</span></span><br><span class="line"><span class="comment"># (&#x27;https://th.wallhaven.cc/small/l8/l83o92.jpg&#x27;, &#x27;https://wallhaven.cc/w/l83o92&#x27;, &#x27;1920 x 1200&#x27;)</span></span><br><span class="line"><span class="comment"># (&#x27;https://th.wallhaven.cc/small/85/85o67y.jpg&#x27;, &#x27;https://wallhaven.cc/w/85o67y&#x27;, &#x27;825 x 1400&#x27;)</span></span><br><span class="line"><span class="comment"># ... ...</span></span><br></pre></td></tr></table></figure>
<p><strong>方式二 bs4</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. 准备数据</span></span><br><span class="line">url = <span class="string">&#x27;https://wallhaven.cc/toplist&#x27;</span></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/105.0.0.0 Safari/537.36&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line">resp = requests.get(url,headers=headers)</span><br><span class="line">resp.encoding = resp.apparent_encoding</span><br><span class="line">page_content = resp.text</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 数据解析</span></span><br><span class="line"><span class="comment"># select()选择器定位  find_all()标签名+属性定位 他们都返回一个列表 |find()返回字符串类型</span></span><br><span class="line"><span class="comment"># [attr]不能直接从列表中获取属性的值</span></span><br><span class="line">soup = BeautifulSoup(page_content,<span class="string">&#x27;lxml&#x27;</span>)</span><br><span class="line">figure_list = soup.select(<span class="string">&#x27;figure.thumb&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> figure <span class="keyword">in</span> figure_list:</span><br><span class="line"></span><br><span class="line">    img_data_src = figure.find(<span class="string">&#x27;img&#x27;</span>)[<span class="string">&#x27;data-src&#x27;</span>]</span><br><span class="line">    <span class="built_in">print</span>(img_data_src)</span><br><span class="line"></span><br><span class="line">    a_href = figure.find(<span class="string">&#x27;a&#x27;</span>)[<span class="string">&#x27;href&#x27;</span>]</span><br><span class="line">    <span class="built_in">print</span>(a_href)</span><br><span class="line"></span><br><span class="line">    span = figure.find(<span class="string">&#x27;span&#x27;</span>).text     <span class="comment">#text获取标签内所有文本 string获取标签内直系文本</span></span><br><span class="line">    <span class="built_in">print</span>(span)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出结果</span></span><br><span class="line"><span class="comment"># https://th.wallhaven.cc/small/l8/l83o92.jpg</span></span><br><span class="line"><span class="comment"># https://wallhaven.cc/w/l83o92</span></span><br><span class="line"><span class="comment"># 1920 x 1200</span></span><br><span class="line"><span class="comment"># https://th.wallhaven.cc/small/85/85o67y.jpg</span></span><br><span class="line"><span class="comment"># https://wallhaven.cc/w/85o67y</span></span><br><span class="line"><span class="comment"># 825 x 1400</span></span><br><span class="line"><span class="comment"># ... ... </span></span><br></pre></td></tr></table></figure>
<p><strong>方式三 xpath</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. 准备数据</span></span><br><span class="line">url = <span class="string">&#x27;https://wallhaven.cc/toplist&#x27;</span></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/105.0.0.0 Safari/537.36&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line">resp = requests.get(url,headers=headers)</span><br><span class="line">resp.encoding = resp.apparent_encoding</span><br><span class="line">page_content = resp.text</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 数据解析</span></span><br><span class="line">tree = etree.HTML(page_content)</span><br><span class="line">figure_list = tree.xpath(<span class="string">&#x27;//figure&#x27;</span>)    <span class="comment">#在文档中查找figure标签，注意xpath()方法永远返回一个列表</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> figure <span class="keyword">in</span> figure_list:</span><br><span class="line">    img_data_src = figure.xpath(<span class="string">&#x27;./img/@data-src&#x27;</span>)[<span class="number">0</span>]  <span class="comment">#获取当前标签下的img标签的data-src属性的值</span></span><br><span class="line">    <span class="built_in">print</span>(img_data_src)</span><br><span class="line"></span><br><span class="line">    a_href = figure.xpath(<span class="string">&#x27;./a/@href&#x27;</span>)[<span class="number">0</span>]</span><br><span class="line">    <span class="built_in">print</span>(a_href)</span><br><span class="line"></span><br><span class="line">    span = figure.xpath(<span class="string">&#x27;.//span[@class=&quot;wall-res&quot;]/text()&#x27;</span>)[<span class="number">0</span>]  <span class="comment"># //在多个层级下寻找 /text()获取标签内的文本</span></span><br><span class="line">    <span class="built_in">print</span>(span)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出结果</span></span><br><span class="line"><span class="comment"># https://th.wallhaven.cc/small/l8/l83o92.jpg</span></span><br><span class="line"><span class="comment"># https://wallhaven.cc/w/l83o92</span></span><br><span class="line"><span class="comment"># 1920 x 1200</span></span><br><span class="line"><span class="comment"># https://th.wallhaven.cc/small/85/85o67y.jpg</span></span><br><span class="line"><span class="comment"># https://wallhaven.cc/w/85o67y</span></span><br><span class="line"><span class="comment"># 825 x 1400</span></span><br><span class="line"><span class="comment"># ... ... </span></span><br></pre></td></tr></table></figure>
<h2 id="爬虫实战（进阶）">爬虫实战（进阶）</h2>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.staticaly.com/gh/sunday521/postimg.wyun521.top@main/img/2023/20230113154149.png" alt="20230113154149"></p>
<p>观察发现，每个图片都放在一个li标签中</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.staticaly.com/gh/sunday521/postimg.wyun521.top@main/img/2023/20230113160905.png" alt="20230113160905"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取整个网页源代码 :给我一个url，返回页面源代码</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">getPageContent</span>(<span class="params">url</span>):</span><br><span class="line">    resp = requests.get(url,headers=headers)</span><br><span class="line">    resp.encoding = resp.apparent_encoding  <span class="comment">#解决页面中文乱码,一定要在resp.text前调用</span></span><br><span class="line">    page_content = resp.text</span><br><span class="line">    <span class="keyword">return</span> page_content;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 从父页面源代码中解析出每个子页面的url</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">jiexi</span>(<span class="params">ex,page_content</span>):</span><br><span class="line">    child_a_list = re.findall(ex,page_content,re.S)</span><br><span class="line">    child_a_list = child_a_list[<span class="number">1</span>:]</span><br><span class="line">    <span class="keyword">return</span> child_a_list;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 从子页面源代码中解析出图片地址</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">jiexi2</span>(<span class="params">ex,page_content</span>):</span><br><span class="line">    img_a_list = re.findall(ex, page_content, re.S)</span><br><span class="line">    <span class="keyword">return</span> img_a_list;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">saveImg</span>(<span class="params"><span class="built_in">dir</span>,img_url</span>):</span><br><span class="line">    nowtime = time.strftime(<span class="string">&quot;%Y%m%d-%H%M%S&quot;</span>, time.localtime())</span><br><span class="line">    filename = <span class="built_in">dir</span> + <span class="string">&#x27;\\&#x27;</span> + nowtime + <span class="string">&#x27;.jpg&#x27;</span></span><br><span class="line"></span><br><span class="line">    resp = requests.get(img_url, headers=headers)</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(filename, <span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> fp:</span><br><span class="line">        fp.write(resp.content)</span><br><span class="line">        <span class="built_in">print</span>(filename, <span class="string">&#x27;保存成功!&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">url = <span class="string">f&#x27;https://pic.netbian.com/4kmeinv/index_2.html&#x27;</span></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/105.0.0.0 Safari/537.36&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">ex = <span class="string">&#x27;&lt;li&gt;&lt;a href=&quot;(.*?)&quot; .*?&lt;b&gt;.*?&lt;/b&gt;&lt;/a&gt;&lt;/li&gt;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># dir = input(&#x27;请输入要将图片保存在那个路径下（如D:\\img）：&#x27;)</span></span><br><span class="line"><span class="built_in">dir</span> = <span class="string">&#x27;C:\\Users\\编程小白\\Pictures\\photo\\bian2&#x27;</span></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(<span class="built_in">dir</span>):</span><br><span class="line">    os.mkdir(<span class="built_in">dir</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;该文件夹已经存在，不需要再创建&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> page <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>,<span class="number">22</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;正在爬取第&#x27;</span>, page, <span class="string">&#x27;页图片...&#x27;</span>)</span><br><span class="line">    url = <span class="string">f&#x27;https://pic.netbian.com/4kdongwu/index_<span class="subst">&#123;page&#125;</span>.html&#x27;</span></span><br><span class="line">    page_content = getPageContent(url)</span><br><span class="line">    child_a_list = jiexi(ex, page_content)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> child_a <span class="keyword">in</span> child_a_list:</span><br><span class="line">        child_url = <span class="string">&#x27;https://pic.netbian.com&#x27;</span>+ child_a</span><br><span class="line">        child_page_content = getPageContent(child_url)</span><br><span class="line">        child_ex = <span class="string">&#x27;&lt;a href=&quot;&quot; id=&quot;img&quot;&gt;&lt;img src=&quot;(.*?)&quot;&#x27;</span></span><br><span class="line">        img_a_list = jiexi2(child_ex,child_page_content)          <span class="comment"># 获取父页面一页所有图片链接</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> img_a <span class="keyword">in</span> img_a_list:</span><br><span class="line">            img_url = <span class="string">&#x27;https://pic.netbian.com&#x27;</span> + img_a</span><br><span class="line">            <span class="built_in">print</span>(img_url)</span><br><span class="line">            saveImg(<span class="built_in">dir</span>,img_url)</span><br><span class="line"></span><br><span class="line">    time.sleep(<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="https://blog.wyun521.top">星期天</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://blog.wyun521.top/posts/c386cf1b/">https://blog.wyun521.top/posts/c386cf1b/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://blog.wyun521.top" target="_blank">星期天の博客</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/python/">python</a><a class="post-meta__tags" href="/tags/%E7%88%AC%E8%99%AB/">爬虫</a></div><div class="post_share"><div class="social-share" data-image="https://mysource-hexo.wyun521.top/img/cover/s9.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i> 打赏</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="https://mysource-hexo.wyun521.top/img/my/wechat.jpg" target="_blank"><img class="post-qr-code-img" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://mysource-hexo.wyun521.top/img/my/wechat.jpg" alt="微信"/></a><div class="post-qr-code-desc">微信</div></li><li class="reward-item"><a href="https://mysource-hexo.wyun521.top/img/my/alipay.jpg" target="_blank"><img class="post-qr-code-img" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://mysource-hexo.wyun521.top/img/my/alipay.jpg" alt="支付宝"/></a><div class="post-qr-code-desc">支付宝</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/posts/4509351/" title="Java集合框架"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://mysource-hexo.wyun521.top/img/cover/s17.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Java集合框架</div></div></a></div><div class="next-post pull-right"><a href="/posts/3778293/" title="Java多线程与并发编程"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://mysource-hexo.wyun521.top/img/cover/s1.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Java多线程与并发编程</div></div></a></div></nav><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="twikoo-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%9D%E8%AF%86%E7%88%AC%E8%99%AB"><span class="toc-text">初识爬虫</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%88%AC%E8%99%AB%E7%9A%84%E6%A6%82%E5%BF%B5"><span class="toc-text">爬虫的概念</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%88%AC%E8%99%AB%E7%9A%84%E5%88%86%E7%B1%BB"><span class="toc-text">爬虫的分类</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BD%91%E7%BB%9C%E8%AF%B7%E6%B1%82%E4%B8%8E%E5%93%8D%E5%BA%94"><span class="toc-text">网络请求与响应</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#http%E5%8D%8F%E8%AE%AE"><span class="toc-text">http协议</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#requests%E6%A8%A1%E5%9D%97"><span class="toc-text">requests模块</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E8%A7%A3%E6%9E%90"><span class="toc-text">数据解析</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F"><span class="toc-text">使用正则表达式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8BeautifulSoup"><span class="toc-text">使用BeautifulSoup</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8xpath%E8%A7%A3%E6%9E%90"><span class="toc-text">使用xpath解析</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E8%A7%A3%E6%9E%90%E5%AE%9E%E6%88%98"><span class="toc-text">数据解析实战</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BD%BC%E5%B2%B8%E5%9B%BE%E7%BD%91"><span class="toc-text">彼岸图网</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Wallhaven"><span class="toc-text">Wallhaven</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%88%AC%E8%99%AB%E5%AE%9E%E6%88%98%EF%BC%88%E8%BF%9B%E9%98%B6%EF%BC%89"><span class="toc-text">爬虫实战（进阶）</span></a></li></ol></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="footer_custom_text">I wish you to become your own sun, no need to rely on who's light.⚽</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div id="algolia-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="search-wrap"><div id="algolia-search-input"></div><hr/><div id="algolia-search-results"><div id="algolia-hits"></div><div id="algolia-pagination"></div><div id="algolia-info"><div class="algolia-stats"></div><div class="algolia-poweredBy"></div></div></div></div></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js"></script><script>function panguFn () {
  if (typeof pangu === 'object') pangu.autoSpacingPage()
  else {
    getScript('https://cdn.jsdelivr.net/npm/pangu/dist/browser/pangu.min.js')
      .then(() => {
        pangu.autoSpacingPage()
      })
  }
}

function panguInit () {
  if (false){
    GLOBAL_CONFIG_SITE.isPost && panguFn()
  } else {
    panguFn()
  }
}

document.addEventListener('DOMContentLoaded', panguInit)</script><script src="https://cdn.jsdelivr.net/npm/algoliasearch/dist/algoliasearch-lite.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instantsearch.js/dist/instantsearch.production.min.js"></script><script src="/js/search/algolia.js"></script><div class="js-pjax"><script>(()=>{
  const init = () => {
    twikoo.init(Object.assign({
      el: '#twikoo-wrap',
      envId: 'https://twikoo.wyun521.top/',
      region: '',
      onCommentLoaded: function () {
        btf.loadLightbox(document.querySelectorAll('#twikoo .tk-content img:not(.tk-owo-emotion)'))
      }
    }, null))
  }

  const getCount = () => {
    const countELement = document.getElementById('twikoo-count')
    if(!countELement) return
    twikoo.getCommentsCount({
      envId: 'https://twikoo.wyun521.top/',
      region: '',
      urls: [window.location.pathname],
      includeReply: false
    }).then(function (res) {
      countELement.innerText = res[0].count
    }).catch(function (err) {
      console.error(err);
    });
  }

  const runFn = () => {
    init()
    
  }

  const loadTwikoo = () => {
    if (typeof twikoo === 'object') {
      setTimeout(runFn,0)
      return
    } 
    getScript('https://cdn.jsdelivr.net/npm/twikoo/dist/twikoo.all.min.js').then(runFn)
  }

  if ('Twikoo' === 'Twikoo' || !true) {
    if (true) btf.loadComment(document.getElementById('twikoo-wrap'), loadTwikoo)
    else loadTwikoo()
  } else {
    window.loadOtherComment = () => {
      loadTwikoo()
    }
  }
})()</script></div><script src="/js/custom@1.0/sunday.js"></script><div class="aplayer no-destroy" data-id="8782402608" data-server="tencent" data-type="playlist" data-loop="all" data-order="random" data-fixed="true" data-mini="true" data-listFolded="false" data-order="random" data-preload="auto" data-autoplay="false" muted></div><canvas id="universe"></canvas><script defer src="/js/custom@1.0/universe.js"></script><script async src="https://cdn1.tianli0.top/npm/vue@2.6.14/dist/vue.min.js"></script><script async src="https://cdn1.tianli0.top/npm/element-ui@2.15.6/lib/index.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = true;
POWERMODE.mobile = false;
document.body.addEventListener('input', POWERMODE);
</script><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/click-heart.min.js" async="async" mobile="true"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/metingjs/dist/Meting.min.js"></script><script src="https://cdn.jsdelivr.net/npm/pjax/pjax.min.js"></script><script>let pjaxSelectors = ["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"]

var pjax = new Pjax({
  elements: 'a:not([target="_blank"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {

  // removeEventListener scroll 
  window.tocScrollFn && window.removeEventListener('scroll', window.tocScrollFn)
  window.scrollCollect && window.removeEventListener('scroll', scrollCollect)

  document.getElementById('rightside').style.cssText = "opacity: ''; transform: ''"
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')

  typeof disqusjs === 'object' && disqusjs.destroy()
})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof chatBtnFn === 'function' && chatBtnFn()
  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', '', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()
})

document.addEventListener('pjax:error', (e) => {
  if (e.request.status === 404) {
    pjax.loadUrl('/404.html')
  }
})</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><!-- hexo injector body_end start --><div class="js-pjax"><script async="async">var arr = document.getElementsByClassName('recent-post-item');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__zoomIn');
    arr[i].setAttribute('data-wow-duration', '1.2s');
    arr[i].setAttribute('data-wow-delay', '0.4s');
    arr[i].setAttribute('data-wow-offset', '100');
    arr[i].setAttribute('data-wow-iteration', '1');
  }</script><script async="async">var arr = document.getElementsByClassName('card-widget');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__zoomIn');
    arr[i].setAttribute('data-wow-duration', '');
    arr[i].setAttribute('data-wow-delay', '');
    arr[i].setAttribute('data-wow-offset', '');
    arr[i].setAttribute('data-wow-iteration', '');
  }</script><script async="async">var arr = document.getElementsByClassName('pagination');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__zoomIn');
    arr[i].setAttribute('data-wow-duration', '');
    arr[i].setAttribute('data-wow-delay', '');
    arr[i].setAttribute('data-wow-offset', '');
    arr[i].setAttribute('data-wow-iteration', '');
  }</script></div><script defer src="https://npm.elemecdn.com/hexo-butterfly-wowjs/lib/wow.min.js"></script><script defer src="https://npm.elemecdn.com/hexo-butterfly-wowjs/lib/wow_init.js"></script><script data-pjax>
  function butterfly_swiper_injector_config(){
    var parent_div_git = document.getElementById('recent-posts');
    var item_html = '<div class="recent-post-item" style="height: auto;width: 100%"><div class="blog-slider swiper-container-fade swiper-container-horizontal" id="swiper_container"><div class="blog-slider__wrp swiper-wrapper" style="transition-duration: 0ms;"><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" onclick="pjax.loadUrl(&quot;posts/c386cf1b/&quot;);" href="javascript:void(0);" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://mysource-hexo.wyun521.top/img/cover/s9.jpg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-01-13</span><a class="blog-slider__title" onclick="pjax.loadUrl(&quot;posts/c386cf1b/&quot;);" href="javascript:void(0);" alt="">Python爬虫</a><div class="blog-slider__text">爬虫是通过编写程序，模拟浏览器上网，并抓取有价值的数据的过程</div><a class="blog-slider__button" onclick="pjax.loadUrl(&quot;posts/c386cf1b/&quot;);" href="javascript:void(0);" alt="">详情       </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" onclick="pjax.loadUrl(&quot;posts/3778293/&quot;);" href="javascript:void(0);" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://mysource-hexo.wyun521.top/img/cover/s1.jpg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-01-08</span><a class="blog-slider__title" onclick="pjax.loadUrl(&quot;posts/3778293/&quot;);" href="javascript:void(0);" alt="">Java多线程与并发编程</a><div class="blog-slider__text">再怎么看我也不知道怎么描述它的啦！</div><a class="blog-slider__button" onclick="pjax.loadUrl(&quot;posts/3778293/&quot;);" href="javascript:void(0);" alt="">详情       </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" onclick="pjax.loadUrl(&quot;posts/c6dc2b94/&quot;);" href="javascript:void(0);" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://mysource-hexo.wyun521.top/img/cover/s11.jpg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-01-23</span><a class="blog-slider__title" onclick="pjax.loadUrl(&quot;posts/c6dc2b94/&quot;);" href="javascript:void(0);" alt="">PS实用教程</a><div class="blog-slider__text">PS的一些实用技能，满足日常修图需求</div><a class="blog-slider__button" onclick="pjax.loadUrl(&quot;posts/c6dc2b94/&quot;);" href="javascript:void(0);" alt="">详情       </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" onclick="pjax.loadUrl(&quot;posts/203dfdcf/&quot;);" href="javascript:void(0);" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://mysource-hexo.wyun521.top/img/cover/s8.jpg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-01-05</span><a class="blog-slider__title" onclick="pjax.loadUrl(&quot;posts/203dfdcf/&quot;);" href="javascript:void(0);" alt="">Markdown文章写作模板</a><div class="blog-slider__text">再怎么看我也不知道怎么描述它的啦！</div><a class="blog-slider__button" onclick="pjax.loadUrl(&quot;posts/203dfdcf/&quot;);" href="javascript:void(0);" alt="">详情       </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" onclick="pjax.loadUrl(&quot;posts/a231cf40/&quot;);" href="javascript:void(0);" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://wallpaper.wyun521.top/唯美/20230112-171126.jpg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-01-06</span><a class="blog-slider__title" onclick="pjax.loadUrl(&quot;posts/a231cf40/&quot;);" href="javascript:void(0);" alt="">Github使用指南</a><div class="blog-slider__text">从项目寻找，筛选，下载，运行一步到位，去Github上探索自己心仪的开源项目吧！</div><a class="blog-slider__button" onclick="pjax.loadUrl(&quot;posts/a231cf40/&quot;);" href="javascript:void(0);" alt="">详情       </a></div></div></div><div class="blog-slider__pagination swiper-pagination-clickable swiper-pagination-bullets"></div></div></div>';
    console.log('已挂载butterfly_swiper')
    parent_div_git.insertAdjacentHTML("afterbegin",item_html)
    }
  var elist = 'undefined'.split(',');
  var cpage = location.pathname;
  var epage = 'all';
  var flag = 0;

  for (var i=0;i<elist.length;i++){
    if (cpage.includes(elist[i])){
      flag++;
    }
  }

  if ((epage ==='all')&&(flag == 0)){
    butterfly_swiper_injector_config();
  }
  else if (epage === cpage){
    butterfly_swiper_injector_config();
  }
  </script><script defer src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.js"></script><script defer data-pjax src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper_init.js"></script><script async src="//at.alicdn.com/t/font_2032782_8d5kxvn09md.js"></script><!-- hexo injector body_end end --></body></html>